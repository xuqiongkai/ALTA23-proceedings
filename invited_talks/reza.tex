Effective real-world NLP systems need to be trustworthy and take users’ contexts into account. As such, they need to recognise and adhere to cultural norms, be able to explain their outcomes, and be capable of complex reasoning with certified knowledge. In this talk, we describe our research towards these goals. First, we discuss the challenges of adhering to socio-cultural norms in cross-cultural and multilingual communications. We present our work on assistive dialogue systems that can identify and address norm violations. Second, we highlight the importance of explaining the outcomes of NLP models while considering the user context. We present methods for generating fast, high-quality, and context-aware explanations. Finally, we look into the limitations of large language models (LLMs) in knowledge hallucination and complex reasoning. We propose a systematic approach to assess LLMs’ knowledge using knowledge graphs and enhance their complex reasoning capabilities for improved accuracy and trustworthiness.
